from .training import Trainer, fetch_dataset, fetch_datasplitter, fetch_model
from .aggregation import aggregate
from .attack import attack
from dataclasses import dataclass
import torch
import torch.nn.functional as F
from loguru import logger
from typing import Literal
import polars as pl
from datetime import datetime
import os
from sklearn.linear_model import Ridge, ElasticNet
from math import ceil
import numpy as np

TEMPERETURE = 0.1


@dataclass
class ExperimentConfig:
    exp_name: str
    model: str
    dataset: str
    split: str
    learning_rate: float
    n_client: int
    m_client: int
    n_server: int
    m_server: int
    device: str
    datapath: str
    batch_size: int
    n_epoch: int
    n_round: int
    num_workers: int
    attack: str
    aggregation: str
    selection_fraction: float
    method: Literal["ours", "baseline"]
    dir_alpha: float | None = None


class Experiment:
    def __init__(self, config: ExperimentConfig) -> None:
        train_set, self.test_set = fetch_dataset(config.datapath, config.dataset)

        self.train_subsets = fetch_datasplitter(
            train_set, config.split, config.n_client, alpha=config.dir_alpha
        ).split()

        self.n_epoch = config.n_epoch
        self.n_round = config.n_round
        self.attack = config.attack
        self.m_client = config.m_client
        self.n_client = config.n_client
        self.n_server = config.n_server
        self.m_server = config.m_server
        self.frac = config.selection_fraction
        self.method = config.method
        self.aggregation = config.aggregation
        self.config = config
        # --- æ–°å¢å’Œä¿®æ”¹çš„éƒ¨åˆ† ---
        self.record = []
        self.exp_name = config.exp_name

        self.credit = torch.zeros(self.n_client)
        self.ema_decay = 0.5
        self.history_for_regression = []
        self.max_history_size = 500
        self.credit_model = Ridge(alpha=1.0)
        self.retrain_interval = 5

        # å®šä¹‰ç»“æœæ–‡ä»¶çš„è·¯å¾„
        self.log_dir = "log"  # å¯ä»¥è‡ªå®šä¹‰æ—¥å¿—ç›®å½•
        os.makedirs(self.log_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.output_file = os.path.join(self.log_dir, f"{self.exp_name}.parquet")

        # å®éªŒå¼€å§‹æ—¶ï¼Œå¯ä»¥é€‰æ‹©æ€§åœ°åˆ é™¤æ—§æ–‡ä»¶ï¼Œä»¥ä¿è¯æ¯æ¬¡è¿è¡Œéƒ½æ˜¯å…¨æ–°çš„ç»“æœ
        # å¦‚æœæ‚¨å¸Œæœ›è¿½åŠ åˆ°æ—§æ–‡ä»¶ä¸­ï¼Œè¯·æ³¨é‡Šæ‰ä¸‹é¢è¿™è¡Œ
        if os.path.exists(self.output_file):
            logger.warning(
                f"Output file {self.output_file} already exists. Removing it for a fresh start."
            )
            os.remove(self.output_file)

        self.reset(config)

    @staticmethod
    def cos_sim_mat(X: torch.Tensor, Y: torch.Tensor):
        X_norm = F.normalize(X, dim=1)
        Y_norm = F.normalize(Y, dim=1)
        return X_norm.matmul(Y_norm.T)

    def reset(self, config: ExperimentConfig):
        init_model = fetch_model(config.model)

        self.local_models = [
            fetch_model(config.model).to(config.device) for _ in range(config.n_client)
        ]

        self.clients = [
            Trainer(
                model=self.local_models[i],
                init_state=init_model.state_dict(),
                train_set=self.train_subsets[i],
                test_set=self.test_set,
                bs=config.batch_size,
                nw=config.num_workers,
                lr=config.learning_rate,
                device=config.device,
            )
            for i in range(config.n_client)
        ]

    def save_results(self, result: dict):
        # å°†å­—å…¸è½¬æ¢ä¸º Polars DataFrame
        results_df = pl.DataFrame(result)

        try:
            # FileLock ä¾ç„¶éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒèƒ½é˜²æ­¢åœ¨â€œè¯»-æ”¹-å†™â€è¿‡ç¨‹ä¸­å‘ç”Ÿç«æ€æ¡ä»¶
            # with FileLock(f'{self.output_file}.lock', timeout=30): # å»ºè®®å¯¹é”æ–‡ä»¶ä½¿ç”¨ä¸åŒæ‰©å±•å
            if os.path.exists(self.output_file):
                # --- æ­£ç¡®çš„è¿½åŠ é€»è¾‘ ---
                # 1. è¯»å–æ—§æ•°æ®
                existing_df = pl.read_parquet(self.output_file)
                # 2. å°†æ–°æ—§æ•°æ®å‚ç›´åˆå¹¶
                combined_df = pl.concat([existing_df, results_df], how="vertical")
                # 3. å°†åˆå¹¶åçš„å®Œæ•´æ•°æ®å†™å›ï¼Œè¦†ç›–åŸæ–‡ä»¶
                combined_df.write_parquet(self.output_file)
                logger.success(f"ğŸ“ˆ Appended 1 record to {self.output_file}")
            else:
                # --- æ–‡ä»¶é¦–æ¬¡åˆ›å»ºçš„é€»è¾‘ ---
                # ç›´æ¥è°ƒç”¨ DataFrame çš„ write_parquet æ–¹æ³•
                results_df.write_parquet(self.output_file)
                logger.success(f"ğŸ’¾ Saved initial record to {self.output_file}")

        except Exception as e:
            logger.error(
                f"âŒ Failed to save results to Parquet file {self.output_file}: {e}"
            )

    def run(self):
        for r in range(self.n_round):
            if self.method == "stateless":
                loss, acc = self.mozi_fl(r, stateful=False)
            elif self.method == "stateful":
                loss, acc = self.mozi_fl(r, stateful=True)
            else:
                loss, acc = self.classic_fl(r)

            record = {
                "exp_name": self.exp_name,
                "timestamp": datetime.now().isoformat(),
                "loss": loss,
                "acc": acc,
                "rnd": r,
            }

            self.save_results(record)

    def classic_fl(self, r: int):
        logger.info(f"Round {r}: Start Training")
        for client in self.clients:
            client.local_train(self.n_epoch)
        logger.info(f"Round {r}: Training End.")

        client_updates = torch.stack([client.get_grad() for client in self.clients])
        client_updates = attack(
            client_updates, self.attack, self.m_client, self.n_client
        )

        server_updates = []
        for i in range(self.n_server):
            if i < self.m_server:
                # when the server is malicious
                update = aggregate(client_updates, "collude", m=self.m_client)
            else:
                # when the server is benign
                update = aggregate(client_updates, "fedavg")
            server_updates.append(update)
        server_updates = torch.stack(server_updates)

        logger.info(f"Round {r}: Aggregation End")
        global_update = aggregate(server_updates, self.aggregation, prop=0.8)

        for client in self.clients:
            client.set_grad(global_update)

        loss, acc = self.clients[-1].test()
        logger.success(f"Round {r}: Loss: {loss:.4f}, Acc: {acc * 100:.2f}!")
        return loss, acc
    
    def mozi_fl(self, r: int, stateful=False):
        # ... (1. è®­ç»ƒä¸æ”»å‡»éƒ¨åˆ†ä¸å˜) ...
        logger.info(f"Round {r}: Start Training")
        for client in self.clients:
            client.local_train(self.n_epoch)
        logger.info(f"Round {r}: Training End.")
        client_updates = torch.stack([client.get_grad() for client in self.clients])
        client_updates = attack(client_updates, self.attack, self.m_client, self.n_client)

        # --- 2. å®¢æˆ·ç«¯é€‰æ‹© ---
        selected_index = self._get_selection_indices(r, stateful)
        logger.info(f"Round {r}: Server Selection:\n {selected_index}.")

        # --- 3. æœåŠ¡å™¨èšåˆä¸è¯„åˆ† ---
        server_updates, scores = self._get_server_updates_and_scores(client_updates, selected_index)

        # --- 4. ä¿¡èª‰ç³»ç»Ÿç®¡ç† (ä»…åœ¨ stateful æ¨¡å¼ä¸‹) ---
        if stateful:
            # 4a. æ”¶é›†æœ¬è½®çš„æ–°è¯æ®
            self._collect_regression_data(scores, selected_index)
            
            # 4b. æ ¹æ®è¯æ®ï¼Œåœ¨éœ€è¦æ—¶æ›´æ–°æˆ‘ä»¬çš„ä¿¡å¿µ
            self._update_credits_if_needed(r)
            
            # 4c. è®°å½•å¹¶æŠ¥å‘Šå½“å‰çš„ä¿¡å¿µçŠ¶æ€
            self._log_credit_stats()

        # --- 5. ç¡®å®šå¹¶åˆ†å‘å…¨å±€æ›´æ–° ---
        winner = scores.argmax()
        logger.success(f"Round {r}: Welcome our new winner: {winner.item()}!")
        global_update = server_updates[winner]

        for client in self.clients:
            client.set_grad(global_update)

        # --- 6. æµ‹è¯•ä¸è¿”å›ç»“æœ ---
        loss, acc = self.clients[-1].test()
        logger.success(f"Round {r}: Loss: {loss:.4f}, Acc: {acc * 100:.2f}!")
        return loss, acc
    
    def _get_selection_indices(self, r: int, stateful: bool) -> torch.Tensor:
        """æ ¹æ®æ¨¡å¼ï¼ˆstateful/statelessï¼‰ç”Ÿæˆå®¢æˆ·ç«¯é€‰æ‹©ç´¢å¼•ã€‚"""
        num_selected = int(round(self.n_client * self.frac))

        if stateful:
            # å®Œå…¨å¤åˆ»ä½ ä»£ç ä¸­çš„â€œå¹³ç§»ä¿¡èª‰â€ç­–ç•¥
            scaled_credit = self.credit.cpu() / TEMPERETURE
            min_credit = scaled_credit.min()
            epsilon = 1e-4
            shifted_credit = scaled_credit - min_credit + epsilon
            credit_prob = shifted_credit / shifted_credit.sum()
            
            logger.info("credit probabilities (sample)", credit_prob)

            if torch.isnan(credit_prob).any():
                logger.warning("Credit probabilities contained NaN. Falling back to uniform selection.")
                credit_prob = torch.ones(self.n_client) / self.n_client

            return torch.stack(
                [
                    torch.multinomial(credit_prob, num_selected, replacement=False)
                    for _ in range(self.n_server)
                ]
            )
        else:
            # Stateless æ¨¡å¼
            return torch.stack(
                [
                    torch.randperm(self.n_client)[:num_selected]
                    for _ in range(self.n_server)
                ]
            )
            
    def _get_server_updates_and_scores(self, client_updates: torch.Tensor, selected_index: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """èšåˆæœåŠ¡å™¨æ›´æ–°å¹¶è®¡ç®—å®ƒä»¬çš„åˆ†æ•°ã€‚"""
        server_updates = []
        for i in range(self.n_server):
            if i < self.m_server:
                update = aggregate(client_updates, "collude", m=self.m_client)
            else:
                update = aggregate(client_updates[selected_index[i]], "fedavg")
            server_updates.append(update)
        server_updates = torch.stack(server_updates)

        similarities = self.cos_sim_mat(server_updates, client_updates)
        scores, _ = similarities.median(dim=1)
        scores = scores.cpu().squeeze()
        
        logger.info(f"Round scores: {scores}")
        return server_updates, scores
    
    def _collect_regression_data(self, scores: torch.Tensor, selected_index: torch.Tensor):
        """
        å¤„ç†å½“å‰è½®æ¬¡çš„æ•°æ®ï¼Œå¹¶å°†å…¶ä½œä¸ºæ–°çš„è®­ç»ƒæ ·æœ¬æ·»åŠ åˆ°å¤šè½®å†å²è®°å½•ä¸­ã€‚
        ä½¿ç”¨EMAå¹³æ»‘çš„ç»Ÿè®¡æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä»¥ä¿è¯è·¨è½®æ¬¡çš„å¯æ¯”æ€§ã€‚
        """
        # 1. è¯†åˆ«æœ¬è½®çš„å¯ä¿¡æœåŠ¡å™¨é›†
        _, sorted_indices = torch.sort(scores)
        num_trusted_servers = max(1, ceil(self.n_server / 2))
        trusted_server_indices = sorted_indices[-num_trusted_servers:]
        logger.info(f"Trusted server set for data collection: {trusted_server_indices.tolist()}")

        # 2. è®¡ç®—æœ¬è½®çš„ä¸´æ—¶ç»Ÿè®¡æ•°æ®
        if trusted_server_indices.numel() == 0:
            logger.warning("No trusted scores found, skipping data collection for this round.")
            return
            
        current_mean = scores.mean().item()
        current_std = scores.std().item()
        stable_std = max(current_std, 1e-6)


        # 4. ä¸ºæœ¬è½®çš„å¯ä¿¡æœåŠ¡å™¨åˆ›å»ºæ ‡å‡†åŒ–åçš„æ•°æ®ç‚¹
        new_data_points = []
        for i in trusted_server_indices:
            participation_vector = np.zeros(self.n_client)
            selected_clients = selected_index[i]
            participation_vector[selected_clients.cpu().numpy()] = 1
            
            raw_score = scores[i].item()
            standardized_score = (raw_score - current_mean) / stable_std
            logger.info(f'regression history add: {participation_vector.tolist()}, {standardized_score}')
            new_data_points.append((participation_vector, standardized_score))

        # 5. å°†æœ¬è½®çš„æ–°æ•°æ®ç‚¹è¿½åŠ åˆ°å¤šè½®å†å²è®°å½•ä¸­
        self.history_for_regression.extend(new_data_points)

        # 6. ç®¡ç†å†å²æ•°æ®å¤§å°
        if len(self.history_for_regression) > self.max_history_size:
            self.history_for_regression = self.history_for_regression[-self.max_history_size:]

    def _update_credits_if_needed(self, r: int):
        """æ£€æŸ¥æ˜¯å¦åˆ°è¾¾å†è®­ç»ƒå‘¨æœŸï¼Œå¦‚æœæ»¡è¶³æ¡ä»¶ï¼Œåˆ™è§¦å‘ä¿¡èª‰æ¨¡å‹çš„å†è®­ç»ƒã€‚"""
        if r > 0 and r % self.retrain_interval == 0 and len(self.history_for_regression) > self.n_client:
            logger.info(f"Round {r}: Retraining credit model")
            self.update_credit_with_regression()

    def _log_credit_stats(self):
        """æ‰“å°å½“å‰è‰¯æ€§ä¸æ¶æ„å®¢æˆ·ç«¯çš„å¹³å‡ä¿¡èª‰ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        benign_avg_credit = self.credit[self.m_client:].mean()
        malicious_avg_credit = self.credit[:self.m_client].mean()
        logger.info(
            f"Benign: {benign_avg_credit:.4f}, Mal: {malicious_avg_credit:.4f}"
        )
    
    def update_credit_with_regression(self):
        """
        ä½¿ç”¨å†å²æ•°æ®å’Œæ ·æœ¬æƒé‡è®­ç»ƒå›å½’æ¨¡å‹ï¼Œå¹¶ç”¨EMAæ›´æ–°å®¢æˆ·ç«¯ä¿¡èª‰ã€‚
        """
        if not self.history_for_regression:
            logger.warning("Regression history is empty. Skipping credit update.")
            return

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X_train = np.array([item[0] for item in self.history_for_regression])
        y_train = np.array([item[1] for item in self.history_for_regression])
        
        # --- æ–¹æ¡ˆä¸€ï¼šåˆ›å»ºæ ·æœ¬æƒé‡ ---
        # ä½¿ç”¨æ ‡å‡†åŒ–å¾—åˆ†çš„ç»å¯¹å€¼ä½œä¸ºæ ·æœ¬æƒé‡ã€‚
        # å¾—åˆ†è¶Šåç¦»å¹³å‡å€¼ï¼ˆæ— è®ºæ˜¯æå¥½è¿˜æ˜¯æåï¼‰ï¼Œè¯¥æ ·æœ¬åœ¨è®­ç»ƒä¸­çš„é‡è¦æ€§å°±è¶Šé«˜ã€‚
        # åŠ ä¸Šä¸€ä¸ªå¾ˆå°çš„å¸¸æ•°ï¼Œä»¥ç¡®ä¿å³ä½¿å¾—åˆ†ä¸º0çš„æ ·æœ¬ä¹Ÿæœ‰ä¸€å®šçš„æƒé‡ã€‚
        sample_weights = np.abs(y_train) + 0.1
        # --- ä¿®æ”¹ç»“æŸ ---

        try:
            # --- æ–¹æ¡ˆä¸€ï¼šå°†æ ·æœ¬æƒé‡ä¼ é€’ç»™ .fit() æ–¹æ³• ---
            self.credit_model.fit(X_train, y_train, sample_weight=sample_weights)
            
            # è·å–æ¨¡å‹æƒé‡ä½œä¸ºæ–°çš„ä¿¡èª‰åˆ†æ•°
            new_credits = torch.from_numpy(self.credit_model.coef_).float()
            print(new_credits)
            # ä½¿ç”¨EMAå¹³æ»‘æ›´æ–°ï¼Œè¿™æ˜¯å®ç°â€œé€æ­¥â€è¾¹ç¼˜åŒ–çš„å…³é”®
            self.credit = self.ema_decay * self.credit.cpu() + \
                          (1 - self.ema_decay) * new_credits
            
            # ä¸ä½¿ç”¨EMA
            # self.credit = new_credits
                          
            logger.success("Credit model retrained and credits updated successfully.")

        except Exception as e:
            logger.error(f"Failed to train credit model: {e}")
            # å¯ä»¥åœ¨è¿™é‡ŒåŠ å…¥è°ƒè¯•ä¿¡æ¯ï¼Œä¾‹å¦‚æ‰“å° X_train.shape, y_train.shape
            logger.error(f"Data shapes: X_train={X_train.shape}, y_train={y_train.shape}")
