from .training import Trainer, fetch_dataset, fetch_datasplitter, fetch_model
from .aggregation import aggregate
from .attack import attack
from dataclasses import dataclass
import torch
import torch.nn.functional as F
from loguru import logger
from typing import Literal
import polars as pl
from datetime import datetime
import os
from sklearn.linear_model import Ridge
from math import ceil
import numpy as np
from scipy.stats import kurtosis

TEMPERATURE = 0.1
IMPORTANCE_TEMPERATURE = 0.1
NUM_SCORES = 2


@dataclass
class ExperimentConfig:
    exp_name: str
    model: str
    dataset: str
    split: str
    learning_rate: float
    n_client: int
    m_client: int
    n_server: int
    m_server: int
    device: str
    datapath: str
    batch_size: int
    n_epoch: int
    n_round: int
    num_workers: int
    attack: str
    aggregation: str
    selection_fraction: float
    method: Literal["stateful", "stateless", "baseline"]
    dir_alpha: float | None = None


class Experiment:
    def __init__(self, config: ExperimentConfig) -> None:
        train_set, self.test_set = fetch_dataset(config.datapath, config.dataset)

        self.train_subsets = fetch_datasplitter(
            train_set, config.split, config.n_client, alpha=config.dir_alpha
        ).split()

        self.n_epoch = config.n_epoch
        self.n_round = config.n_round
        self.attack = config.attack
        self.m_client = config.m_client
        self.n_client = config.n_client
        self.n_server = config.n_server
        self.m_server = config.m_server
        self.frac = config.selection_fraction
        self.method = config.method
        self.aggregation = config.aggregation
        self.config = config
        # --- æ–°å¢å’Œä¿®æ”¹çš„éƒ¨åˆ† ---
        self.record = []
        self.exp_name = config.exp_name

        self.credit = torch.zeros(self.n_client)
        self.ema_decay = 0.5
        self.history_for_regression = []
        self.max_history_size = 500
        self.credit_model = Ridge(alpha=1.0)
        self.retrain_interval = 2
        self.score_importances = torch.ones(NUM_SCORES) / NUM_SCORES
        self.prev_winner = 1
        # å®šä¹‰ç»“æœæ–‡ä»¶çš„è·¯å¾„
        self.log_dir = "log"  # å¯ä»¥è‡ªå®šä¹‰æ—¥å¿—ç›®å½•
        os.makedirs(self.log_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.output_file = os.path.join(self.log_dir, f"{self.exp_name}.parquet")

        # å®éªŒå¼€å§‹æ—¶ï¼Œå¯ä»¥é€‰æ‹©æ€§åœ°åˆ é™¤æ—§æ–‡ä»¶ï¼Œä»¥ä¿è¯æ¯æ¬¡è¿è¡Œéƒ½æ˜¯å…¨æ–°çš„ç»“æœ
        # å¦‚æœæ‚¨å¸Œæœ›è¿½åŠ åˆ°æ—§æ–‡ä»¶ä¸­ï¼Œè¯·æ³¨é‡Šæ‰ä¸‹é¢è¿™è¡Œ
        if os.path.exists(self.output_file):
            logger.warning(
                f"Output file {self.output_file} already exists. Removing it for a fresh start."
            )
            os.remove(self.output_file)

        self.reset(config)

    @staticmethod
    def cos_sim_mat(X: torch.Tensor, Y: torch.Tensor):
        X_norm = F.normalize(X, dim=1)
        Y_norm = F.normalize(Y, dim=1)
        return X_norm.matmul(Y_norm.T)

    @staticmethod
    def _calc_kurtosis(x: torch.Tensor):
        x_np = x.cpu().numpy()
        return torch.from_numpy(kurtosis(x_np, axis=1, fisher=False)).to(x.device)

    def reset(self, config: ExperimentConfig):
        init_model = fetch_model(config.model)

        self.local_models = [
            fetch_model(config.model).to(config.device) for _ in range(config.n_client)
        ]

        self.clients = [
            Trainer(
                model=self.local_models[i],
                init_state=init_model.state_dict(),
                train_set=self.train_subsets[i],
                test_set=self.test_set,
                bs=config.batch_size,
                nw=config.num_workers,
                lr=config.learning_rate,
                device=config.device,
            )
            for i in range(config.n_client)
        ]

    def save_results(self, result: dict):
        # å°†å­—å…¸è½¬æ¢ä¸º Polars DataFrame
        results_df = pl.DataFrame(result)

        try:
            # FileLock ä¾ç„¶éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒèƒ½é˜²æ­¢åœ¨â€œè¯»-æ”¹-å†™â€è¿‡ç¨‹ä¸­å‘ç”Ÿç«æ€æ¡ä»¶
            # with FileLock(f'{self.output_file}.lock', timeout=30): # å»ºè®®å¯¹é”æ–‡ä»¶ä½¿ç”¨ä¸åŒæ‰©å±•å
            if os.path.exists(self.output_file):
                # --- æ­£ç¡®çš„è¿½åŠ é€»è¾‘ ---
                # 1. è¯»å–æ—§æ•°æ®
                existing_df = pl.read_parquet(self.output_file)
                # 2. å°†æ–°æ—§æ•°æ®å‚ç›´åˆå¹¶
                combined_df = pl.concat([existing_df, results_df], how="vertical")
                # 3. å°†åˆå¹¶åçš„å®Œæ•´æ•°æ®å†™å›ï¼Œè¦†ç›–åŸæ–‡ä»¶
                combined_df.write_parquet(self.output_file)
                logger.success(f"ğŸ“ˆ Appended 1 record to {self.output_file}")
            else:
                # --- æ–‡ä»¶é¦–æ¬¡åˆ›å»ºçš„é€»è¾‘ ---
                # ç›´æ¥è°ƒç”¨ DataFrame çš„ write_parquet æ–¹æ³•
                results_df.write_parquet(self.output_file)
                logger.success(f"ğŸ’¾ Saved initial record to {self.output_file}")

        except Exception as e:
            logger.error(
                f"âŒ Failed to save results to Parquet file {self.output_file}: {e}"
            )

    def run(self):
        for r in range(self.n_round):
            if self.method == "stateless":
                loss, acc = self.mozi_fl(r, stateful=False)
            elif self.method == "stateful":
                loss, acc = self.mozi_fl(r, stateful=True)
            else:
                loss, acc = self.classic_fl(r)

            record = {
                "exp_name": self.exp_name,
                "timestamp": datetime.now().isoformat(),
                "loss": loss,
                "acc": acc,
                "rnd": r,
            }

            self.save_results(record)

    def classic_fl(self, r: int):
        logger.info(f"Round {r}: Start Training")
        for client in self.clients:
            client.local_train(self.n_epoch)
        logger.info(f"Round {r}: Training End.")

        client_updates = torch.stack([client.get_grad() for client in self.clients])
        client_updates = attack(
            client_updates, self.attack, self.m_client, self.n_client
        )

        server_updates = []
        for i in range(self.n_server):
            if i < self.m_server:
                # when the server is malicious
                update = aggregate(client_updates, "collude", m=self.m_client)
            else:
                # when the server is benign
                update = aggregate(client_updates, "fedavg")
            server_updates.append(update)
        server_updates = torch.stack(server_updates)

        logger.info(f"Round {r}: Aggregation End")
        global_update = aggregate(server_updates, self.aggregation, prop=0.8)

        for client in self.clients:
            client.set_grad(global_update)

        loss, acc = self.clients[-1].test()
        logger.success(f"Round {r}: Loss: {loss:.4f}, Acc: {acc * 100:.2f}!")
        return loss, acc

    def mozi_fl(self, r: int, stateful=False):
        # local training and simulating attacks
        logger.info(f"Round {r}: Start Training")
        for client in self.clients:
            client.local_train(self.n_epoch)
        logger.info(f"Round {r}: Training End.")
        client_updates = torch.stack([client.get_grad() for client in self.clients])
        client_updates = attack(
            client_updates, self.attack, self.m_client, self.n_client
        )

        # select client sbubsets
        selected_index = self._select_clients(
            num_selected=int(self.frac * self.n_client), temperature=0.3
        )
        logger.info(f"Round {r}: Server Selection:\n {selected_index}.")

        # aggregate and score
        server_updates = self._get_server_updates(client_updates, selected_index)

        scores = self._calc_scores(client_updates, server_updates)

        self._collect_regression_data(scores, selected_index)
        self._update_credits_if_needed(r)
        self._log_credit_stats()

        composite_scores = (scores * self.score_importances).sum(dim=1)

        # identify and broadcast the `winner` update
        winner = composite_scores.argmax()
        logger.success(f"Round {r}: Welcome our new winner: {winner.item()}!")
        global_update = server_updates[winner]
        self.prev_winner = winner
        for client in self.clients:
            client.set_grad(global_update)

        # test and log
        loss, acc = self.clients[-1].test()
        logger.success(f"Round {r}: Loss: {loss:.4f}, Acc: {acc * 100:.2f}!")
        return loss, acc

    def _select_clients(
        self, num_selected: int, temperature: float = 0.1
    ) -> torch.Tensor:
        """
        ä¸ºæ¯ä¸ªæœåŠ¡å™¨é€‰æ‹©å®¢æˆ·ç«¯ã€‚
        å¯¹äº prev_winnerï¼Œä½¿ç”¨ softmax æ¦‚ç‡é‡‡æ ·ã€‚
        å¯¹äºå…¶ä»–æœåŠ¡å™¨ï¼Œä½¿ç”¨éšæœºé‡‡æ ·ã€‚

        Args:
            num_selected (int): è¦é€‰æ‹©çš„å®¢æˆ·ç«¯æ•°é‡ã€‚
            temperature (float): Softmax çš„æ¸©åº¦å‚æ•°ã€‚
        """
        return torch.stack(
            [
                torch.multinomial(
                    torch.softmax(self.credit, dim=0).cpu() / temperature,
                    num_samples=num_selected,
                    replacement=False,
                )
                for _ in range(self.n_server)
            ]
        )

    def _get_server_updates(
        self, client_updates: torch.Tensor, selected_index: torch.Tensor
    ) -> torch.Tensor:
        server_updates = []
        for i in range(self.n_server):
            if i < self.m_server:
                update = aggregate(client_updates, "collude", m=self.m_client)
            else:
                update = aggregate(client_updates[selected_index[i]], "fedavg")
            server_updates.append(update)
        server_updates = torch.stack(server_updates)

        return server_updates

    def _rescale_scores(self, scores: torch.Tensor) -> torch.Tensor:
        """å¯¹å•æ‰¹æ¬¡å†…çš„åˆ†æ•°è¿›è¡ŒMin-Maxç¼©æ”¾ï¼Œä½¿å…¶åˆ†å¸ƒåœ¨[0, 1]"""
        min_val = torch.min(scores)
        max_val = torch.max(scores)

        # å¤„ç†æ‰€æœ‰å€¼éƒ½ç›¸åŒçš„è¾¹ç¼˜æƒ…å†µï¼Œé¿å…é™¤ä»¥é›¶
        if max_val == min_val:
            # å¯ä»¥è¿”å›å…¨0.5æˆ–å…¨0ï¼Œå–å†³äºä½ çš„åå¥½
            return torch.full_like(scores, 0.5)

        return (scores - min_val) / (max_val - min_val)

    def _calc_scores(self, client_updates: torch.Tensor, server_updates: torch.Tensor):
        """calculate 3 socres between client and server updates"""

        # similarity scores
        cos_scores = self.cos_sim_mat(server_updates, client_updates)

        # magnitude scores
        server_norms = torch.norm(server_updates, p=2, dim=1).unsqueeze(1)
        client_norms = torch.norm(client_updates, p=2, dim=1).unsqueeze(0)
        mag_scores = 1 - torch.abs(client_norms - server_norms) / (
            client_norms + server_norms + 1e-9
        )

        # sign scores
        # server_signs = self._get_sign_stats(server_updates).unsqueeze(1)
        # client_signs = self._get_sign_stats(client_updates).unsqueeze(0)
        # sgn_scores = 1 - torch.abs(client_signs - server_signs)

        cos_scores = self._rescale_scores(cos_scores)
        mag_scores = self._rescale_scores(mag_scores)

        all_scores = torch.stack([cos_scores, mag_scores])
        median_scores, _ = all_scores.median(dim=2)
        logger.info(f"Round scores: {median_scores}")
        return median_scores.T.cpu()

    def _collect_regression_data(
        self, scores: torch.Tensor, selected_index: torch.Tensor
    ):
        """
        Identifies a set of trusted servers for the current round, and only adds
        their participation data and standardized features to the history.
        """
        # probe_features is a (K, d) tensor
        num_probes, num_dims = scores.shape

        # --- 1. è®¡ç®—ä¸´æ—¶çš„ç»¼åˆåˆ†æ•°ä»¥è¯†åˆ«å¯ä¿¡æœåŠ¡å™¨ ---
        # ä½¿ç”¨ä¸ mozi_fl ä¸­ç›¸åŒçš„æƒé‡æ¥ç¡®ä¿ä¸€è‡´æ€§
        composite_scores = (scores * self.score_importances).sum(dim=1)

        # --- 2. è¯†åˆ«æœ¬è½®çš„å¯ä¿¡æœåŠ¡å™¨é›† (æ‚¨çš„åŸå§‹é€»è¾‘) ---
        # æ ¹æ®ç»¼åˆåˆ†æ•°è¿›è¡Œæ’åº
        _, sorted_indices = torch.sort(composite_scores, descending=True)

        # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„ top 50% (æˆ–è‡³å°‘1ä¸ª) ä½œä¸ºå¯ä¿¡é›†
        num_trusted_servers = max(1, ceil(self.n_server / 2))
        trusted_server_indices = sorted_indices[:num_trusted_servers]

        logger.info(
            f"Trusted server set for data collection: {trusted_server_indices.tolist()}"
        )

        if trusted_server_indices.numel() == 0:
            logger.warning("No trusted servers identified. Skipping data collection.")
            return

        # åœ¨è¿™ä¸ªâ€œå¹²å‡€â€çš„æ•°æ®é›†ä¸Šè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        mean = scores.mean(dim=0)
        std = scores.std(dim=0).clamp(min=1e-6)

        # æ ‡å‡†åŒ–æ‰€æœ‰æ¢é’ˆçš„ç‰¹å¾ï¼Œä½†ä½¿ç”¨å¯ä¿¡é›†çš„ç»Ÿè®¡æ•°æ®ä½œä¸ºåŸºå‡†
        # è¿™æ ·ï¼Œå³ä½¿æ˜¯â€œåâ€æ¢é’ˆï¼Œå…¶åˆ†æ•°ä¹Ÿä¼šè¢«è½¬æ¢åˆ°è¿™ä¸ªâ€œå¥½â€çš„åæ ‡ç³»ä¸‹
        standardized_features = (scores - mean) / std

        # --- 4. ä¸ºæœ¬è½®çš„å¯ä¿¡æœåŠ¡å™¨åˆ›å»ºå¹¶æ·»åŠ æ•°æ®ç‚¹ ---
        new_data_points = []
        # åªéå†å¯ä¿¡æœåŠ¡å™¨çš„ç´¢å¼•
        for k in trusted_server_indices:
            participation_vector = np.zeros(self.n_client)
            selected_clients = selected_index[k]
            participation_vector[selected_clients.cpu().numpy()] = 1

            # ç›®æ ‡æ˜¯æ ‡å‡†åŒ–çš„å¤šç»´ç‰¹å¾å‘é‡
            target_vector = standardized_features[k].cpu().numpy()

            new_data_points.append((participation_vector, target_vector))
            logger.debug(
                f"Adding to history: P-Vec (sum={participation_vector.sum()}), Target={np.round(target_vector, 2)}"
            )

        # --- 5. è¿½åŠ åˆ°å†å²è®°å½•å¹¶ç®¡ç†å¤§å° (é€»è¾‘ä¸å˜) ---
        self.history_for_regression.extend(new_data_points)
        if len(self.history_for_regression) > self.max_history_size:
            self.history_for_regression = self.history_for_regression[
                -self.max_history_size :
            ]

    def _update_credits_if_needed(self, r: int):
        """æ£€æŸ¥æ˜¯å¦åˆ°è¾¾å†è®­ç»ƒå‘¨æœŸï¼Œå¦‚æœæ»¡è¶³æ¡ä»¶ï¼Œåˆ™è§¦å‘ä¿¡èª‰æ¨¡å‹çš„å†è®­ç»ƒã€‚"""
        if (
            r > 0
            and r % self.retrain_interval == 0
            and len(self.history_for_regression) > self.n_client
        ):
            logger.info(f"Round {r}: Retraining credit model")
            self.update_credit_with_regression()

    def _log_credit_stats(self):
        """æ‰“å°å½“å‰è‰¯æ€§ä¸æ¶æ„å®¢æˆ·ç«¯çš„å¹³å‡ä¿¡èª‰ç»Ÿè®¡ä¿¡æ¯ã€‚"""
        benign_avg_credit = self.credit[self.m_client :].mean()
        malicious_avg_credit = self.credit[: self.m_client].mean()
        logger.info(f"Benign: {benign_avg_credit:.4f}, Mal: {malicious_avg_credit:.4f}")

    @staticmethod
    def _get_sign_stats(tensor: torch.Tensor) -> torch.Tensor:
        """Calculates the sign statistics (non-negative counts) for a tensor."""
        return (tensor >= 0).float().sum(dim=1) / tensor.shape[1]

    def update_credit_with_regression(self):
        """
        Uses historical data to train a multi-target regression model and updates client credits.
        """
        if (
            not self.history_for_regression
            or len(self.history_for_regression) < self.n_client
        ):
            logger.warning("Not enough history for regression. Skipping credit update.")
            return

        # 1. å‡†å¤‡è®­ç»ƒæ•°æ®
        X_train = np.array(
            [item[0] for item in self.history_for_regression]
        )  # Shape: (T_history, N)
        y_train = np.array(
            [item[1] for item in self.history_for_regression]
        )  # Shape: (T_history, d)

        # æ£€æŸ¥y_trainä¸­æ˜¯å¦æœ‰NaNå€¼ (å¯èƒ½ç”±std=0å¯¼è‡´)
        if np.isnan(y_train).any():
            logger.warning("NaNs found in y_train, filling with 0.")
            y_train = np.nan_to_num(y_train)

        sample_importance = np.linalg.norm(y_train, axis=1) + 0.1
        try:
            # scikit-learn's Ridge seamlessly handles a 2D y_train
            self.credit_model.fit(X_train, y_train, sample_weight=sample_importance)

            # model.coef_ shape will be (d, N), so we transpose it
            new_credits_matrix = torch.from_numpy(
                self.credit_model.coef_.T
            ).float()  # Shape: (N, d)
            # ç®€å•æ±‚å’Œå³å¯ï¼Œå› ä¸ºå²­å›å½’çš„ç³»æ•°å·²ç»åæ˜ äº†æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§
            new_credits_vector = new_credits_matrix.sum(dim=1)  # Shape: (N,)
            self.credit = new_credits_vector

            logger.success("Multi-target credit model retrained. Credits updated.")
            logger.info(
                f"Learned credit matrix :\n{new_credits_matrix}"
            )
            logger.info(
                f"New credit:\n{new_credits_vector}"
            )

        except Exception as e:
            logger.error(f"Failed to train credit model: {e}")
            logger.error(
                f"Data shapes: X_train={X_train.shape}, y_train={y_train.shape}"
            )
